import os
import gradio as gr
import fitz  # Th∆∞ vi·ªán PyMuPDF
from langchain.embeddings.openai import OpenAIEmbeddings
import time
import shutil
import retrievergpt
  

embedding_tool = OpenAIEmbeddings(chunk_size=500,embedding_ctx_length=1024,max_retries=3,model_kwargs={"maxConcurrency" : 2})
local_save = retrievergpt.local_save
vector_save = retrievergpt.vector_save
os.makedirs(os.path.dirname(local_save), exist_ok=True)



##############################################################################################################
# Danh s√°ch ƒë·ªÉ l∆∞u tr·ªØ th√¥ng tin c√°c file ƒë√£ ƒë·ªçc
read_files_info = {}
readed_file=""
index_file=0
docs=[]
vector_space=[]
def add_text(history, text):
    history = history + [(text, "text")]
    return history, gr.Textbox(value="", interactive=False)

def add_file(history, file):
    file_name = file.name.split("\\")[-1]
    local_path = os.path.join(os.getcwd(), local_save)
    local_path = os.path.join(local_path, file_name)
    global read_files_info
    global readed_file
    if file_name not in read_files_info:
        if file.name.lower().endswith('.txt'):
            # ƒê·ªçc n·ªôi dung t·ª´ file vƒÉn b·∫£n
            with open(file.name, 'r', encoding='utf-8') as txt_file:
                text = txt_file.read()
                history = history + [(file_name,"file" )]  # ƒê√°nh d·∫•u l√† file
                read_files_info[file_name] = text
                readed_file=file_name
                global docs
                global vector_space
                shutil.copy(file.name, local_path)
                retrievergpt.preprocess(embedder=embedding_tool, data_path=local_path, vector_path=vector_save)
        elif file.name.lower().endswith('.pdf'):
            # ƒê·ªçc n·ªôi dung t·ª´ file PDF b·∫±ng PyMuPDF
            with fitz.open(file.name) as pdf_document:
                text = ""
                for page_number in range(pdf_document.page_count):
                    page = pdf_document[page_number]
                    text += page.get_text()
                history = history + [( file_name,"file")]  # ƒê√°nh d·∫•u l√† file
                read_files_info[file_name] = text
                readed_file=file_name
                shutil.copy(file.name, local_path)
                retrievergpt.preprocess(embedder=embedding_tool, data_path=local_path, vector_path=vector_save)
        else:
            history = history + [("Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng file tr√™n", None)]
    else:
        history = history + [(file_name, None)]
        if (file_name!=readed_file):
            shutil.copy(file.name, local_path)
            retrievergpt.preprocess(embedder=embedding_tool, data_path=local_path, vector_path=vector_save)
    return history



def bot(history):
    db = retrievergpt.loadVector(embedding_tool)
    response = ""
    # Ki·ªÉm tra lo·∫°i c·ªßa m·ª•c cu·ªëi c√πng trong history
    last_item = history[-1]
    last_item_type = last_item[1]
    if history:
        if last_item_type == "text":
            if len(read_files_info)==0 and db is None:
                response += "B·∫°n vui l√≤ng nh·∫≠p file tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi\n\n"
            else:
                #answer = query(question=last_item[0],vector_space=vector_space,docs=docs,model=model)
                answer = retrievergpt.dbQuery(question=last_item[0],retriever=db,k=3)
                response+=answer
            
        elif last_item_type == "file":
            response += "File b·∫°n v·ª´a t·∫£i l√™n t√¥i ƒë√£ ƒë·ªçc xong\n\n"
        
        elif last_item_type is None:
            if (last_item[0]=="Invalid file format"):
                response+="File b·∫°n v·ª´a nh·∫≠p kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£"
            else:
                response += "File b·∫°n v·ª´a t·∫£i l√™n t√¥i ƒë√£ ƒë·ªçc xong\n\n"
    else:
        response += "D·ªØ li·ªáu kh√¥ng t√¨m ƒë∆∞·ª£c"
   
        
   
    history[-1][1] = response
    time.sleep(0.005)
    yield history



with gr.Blocks() as demo:
    chatbot = gr.Chatbot(
        [],
        elem_id="chatbot",
        bubble_full_width=False,
        
    )

    with gr.Row():
        txt = gr.Textbox(
            scale=4,
            show_label=False,
            placeholder="Nh·∫≠p c√¢u h·ªèi v√†o ƒë√¢y v√† nh·∫•n Enter ƒë·ªÉ tr·∫£ l·ªùi. Ho·∫∑c l√† b·∫•m v√†o üìÅ ƒë·ªÉ upload PDF/TXT",
            container=False,
        )
        btn = gr.UploadButton("üìÅ", file_types=["pdf", "txt"])

    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(
        bot, chatbot, chatbot, api_name="bot_response"
    )
    txt_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)
    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
        bot, chatbot, chatbot
    )

demo.queue()
if __name__ == "__main__":  
    demo.launch()
